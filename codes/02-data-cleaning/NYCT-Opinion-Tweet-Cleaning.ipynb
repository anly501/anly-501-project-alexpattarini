{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for cleaning Opinion Tweet Data\n",
    "\n",
    "Cleaner function adapted from Professor James Hickman's wikipedia crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\alexp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import relevant packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('words')\n",
    "\n",
    "# Import raw tweet data\n",
    "op_df = pd.read_csv(\"../../data/00-raw-data/Opinion-Tweets-1011.csv\",index_col=0,encoding=\"unicode_escape\")\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "# Initialize models\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stop words\n",
    "mystopwords = stopwords.words('english')\n",
    "# Append frequently used words from NYCT twitter that are not useful for analysis\n",
    "mystopwords.extend([\"Northbound\",\"Southbound\",\"southbound\",\"northbound\",\"both\",\"directions\",\"while\",\"running\",\"problems\",\"delays\",\"delayed\",\"delay\",\"st\",\"ave\",\"av\",\"rd\",\"train\",\"trains\",\"near\",'i','d','s','t','need'])\n",
    "mystopwords = set(mystopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADAPTED FROM PROFESSOR HICKMAN'S CLEANER FUNCTION\n",
    "# Define tweet cleaning function\n",
    "def twtclean(tweet):\n",
    "\tnew_text=\"\"\n",
    "    # Chars to keep\n",
    "\tkeep=\" abcdefghijklmnopqrstuvwxyz0123456789\"\n",
    "\tfor char in tweet:\n",
    "\t\tif char.lower() in keep:\n",
    "\t\t\tnew_text+=char.lower()\n",
    "\t\telse: \n",
    "\t\t\tnew_text+=\" \"\n",
    "\ttweet=new_text\n",
    "\t\n",
    "\t# Filter stop words out\n",
    "\tnew_text=\"\"\n",
    "\tfor w in nltk.tokenize.word_tokenize(tweet):\n",
    "\t\tif w not in mystopwords:\n",
    "\t\t\tlem = lemmatizer.lemmatize(w)\n",
    "\t\t\tw = lem\n",
    "\t\t\tif len(w)>1:\n",
    "\t\t\t\tif w in [\".\",\",\",\"!\",\"?\",\":\",\";\"]:\n",
    "\t\t\t\t\t#remove the last space\n",
    "\t\t\t\t\tnew_text=new_text[0:-1]+w+\" \"\n",
    "\t\t\t\telse: #add a space\n",
    "\t\t\t\t\tnew_text+=w.lower()+\" \"\n",
    "\ttweet=new_text.strip()\n",
    "\n",
    "\t# Filter out non words\n",
    "\tnew_text = \"\"\n",
    "\tfor w in nltk.tokenize.word_tokenize(tweet):\n",
    "\t\tif w in words:\n",
    "\t\t\tnew_text += w.lower() + \" \"\n",
    "\ttweet=new_text.strip()\n",
    "\treturn tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract sentiment analysis and clean full text of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text of tweets to list\n",
    "full_text = op_df['full_text'].tolist()\n",
    "# Create corpus to be filled with cleaned tweets and list of sentiments\n",
    "corpus = []\n",
    "sents = []\n",
    "# For loop partially adapted from Professor James Hickman's example\n",
    "for tweet in full_text:\n",
    "    tweet = twtclean(tweet)\n",
    "    # Append to corpus\n",
    "    corpus.append(tweet)\n",
    "    # Sentiment analysis\n",
    "    score = sid.polarity_scores(tweet)\n",
    "    new_sent = score['compound']\n",
    "    # Extract pos/neu/neg label\n",
    "    new_max = 0\n",
    "    new_label = \"\"\n",
    "    for i in range(0,3):\n",
    "        val = abs(list(score.values())[i])\n",
    "        if val > new_max:\n",
    "            new_max = val\n",
    "            new_label = list(score)[i]\n",
    "    sents.append([new_sent,new_label])\n",
    "    #print(score)\n",
    "    #print(new_label)\n",
    "\n",
    "# Save to csv\n",
    "new_df = []\n",
    "for i in range(0,len(corpus)):\n",
    "    new_df.append([corpus[i],sents[i][0],sents[i][1]])\n",
    "df=pd.DataFrame(new_df)\n",
    "df=df.rename(columns={0: \"text\", 1: \"sentiment\", 2: \"category\"})\n",
    "# Drop any rows with NA values\n",
    "df['text'].replace('', np.nan, inplace=True)\n",
    "df = df.dropna()\n",
    "df.to_csv('../../data/01-modified-data/NYCT-Opinion-Tweets-Sentiments.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding (Document Term Frequency Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text of tweets to list\n",
    "tweets = op_df['full_text'].tolist()\n",
    "# Remove urls from text\n",
    "tweets = [re.sub(r'http\\S+', '', x) for x in tweets]\n",
    "# Use CountVectorizer to tokenize tweets\n",
    "nyct_cvec = CountVectorizer(stop_words=mystopwords,min_df=0.001)\n",
    "nyct_mx = nyct_cvec.fit_transform(tweets)\n",
    "nyct_array = nyct_mx.toarray()\n",
    "# Create document term frequency matrix\n",
    "nyct_dtm = pd.DataFrame(data=nyct_array,columns=nyct_cvec.get_feature_names_out())\n",
    "# Save dtm to csv\n",
    "#nyct_dtm.to_csv('../../data/01-modified-data/NYCT-0901-0914-tweets-DTM.csv')\n",
    "nyct_dtm.to_csv('../../data/01-modified-data/Opinion-tweets-DTM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save to html\n",
    "os.system('jupyter nbconvert --to html NYCT-Opinion-Tweet-Cleaning.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ANLY501')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45400336930f63a3dad52b4eda0d8feb5d28fea33ac62eb90fb48ebf47238ba1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

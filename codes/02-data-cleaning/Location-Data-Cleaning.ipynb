{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location Data Cleaning\n",
    "\n",
    "This script cleans and converts the MTA location data into a form that is appropriate for ARM analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initally, the subway location data comprises all of the entrances and exits of each individual subway station, but we are only concerned with the routes associated with each station, so only the columns that identify each station and its accompanying routes are kept. Duplicates are dropped accordingly (they were previously distinguished as the information for different entrances and exits to the stations themselves.). A new variable that is composed of all of the routes for each station must be created, however considering there are 11 columns for different routes, and most stations do not have nearly as many as that (42nd St. Times Square has 11), these NA values must be temporarily filled in to allow for the combination of the columns. These placeholder value, \"X\", will be removed in the final, modified data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data and subset to route columns\n",
    "df = pd.read_csv(\"../../data/00-raw-data/MTA-Subway-Station-Location-Data.csv\",index_col=0)\n",
    "df = df[['division','line','station_name','route1','route2','route3','route4','route5','route6','route7','route8','route9','route10','route11']]\n",
    "df = df.drop_duplicates()\n",
    "# Replace NA route values and convert to str\n",
    "df= df.fillna(\"X\")\n",
    "df = df.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, the columns containing the route info are concatenated to create a \"basket\" of routes for each station (the variable 'allroutes'). The X's and excessive commas are then removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['allroutes'] = df['route1'] + \",\" + df['route2'] + \",\" + df['route3'] + \",\" + df['route4'] + \",\" + df['route5'] + \",\" + df['route6'] + \",\" + df['route7'] + \",\" + df['route8'] + \",\" + df['route9'] + \",\" + df['route10'] + \",\" + df['route11']\n",
    "df['allroutes'] = df['allroutes'].str.replace(\",X\",\"\")\n",
    "# Save dataframe in csv\n",
    "df.to_csv(\"../../data/01-modified-data/Location-Data-Cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\alexp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alexp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alexp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alexp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\alexp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.downloader import download\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from apyori import apriori\n",
    "import networkx as nx \n",
    "\n",
    "download('vader_lexicon')\n",
    "download('stopwords')\n",
    "download('wordnet')\n",
    "download('punkt')\n",
    "download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = list(apriori(df,min_support=0.005,min_confidence=0.004,min_length=1,max_length=9))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ANLY501')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45400336930f63a3dad52b4eda0d8feb5d28fea33ac62eb90fb48ebf47238ba1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
